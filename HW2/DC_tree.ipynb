{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuaan\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Model Random Forest:\n",
      "-------------------------------------------\n",
      "Start implementing holdout_validation:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "        \n",
    "class DecisionTree:\n",
    "    def __init__(self, min_num_of_node = 100, max_depth = 5):\n",
    "        self.min_num_of_node = min_num_of_node\n",
    "        self.max_depth = max_depth\n",
    "        self.root = {}\n",
    "    \n",
    "    def Entropy(self, tar):\n",
    "        s = sum(tar.values())\n",
    "        g = 0\n",
    "        for t in tar.values():\n",
    "            t /= s\n",
    "            g += -(t * math.log2(t) )\n",
    "        return g\n",
    "    def Gini(self, tar):\n",
    "        s = sum(tar.values())\n",
    "        g = 0\n",
    "        for t in tar.values():\n",
    "            t /= s\n",
    "            g += t*t\n",
    "        return (1.0 - g)\n",
    "            \n",
    "    def categorical_split(self, attr, X_idx, data_X, target):\n",
    "        level = {}\n",
    "        length = {}\n",
    "        sp = {}\n",
    "        T = {}\n",
    "        for dx in X_idx:\n",
    "                \n",
    "            #levels\n",
    "            att = data_X.loc[dx, attr]\n",
    "            if att == ' ?':\n",
    "                continue\n",
    "            if att not in level:\n",
    "                level[att] = {}\n",
    "                length[att] = 0\n",
    "                sp[att] = []\n",
    "                \n",
    "            length[att] += 1\n",
    "            sp[att].append(dx)\n",
    "            tar = target.loc[dx,'Category']\n",
    "            if tar not in T:\n",
    "                T[tar] = 0\n",
    "            T[tar] += 1\n",
    "            \n",
    "            if tar not in level[att]:\n",
    "                level[att][tar] = 0\n",
    "            level[att][tar] += 1\n",
    "        \n",
    "        HD = self.Entropy(length)\n",
    "        if HD == 0:\n",
    "            return -999999, {}\n",
    "        \n",
    "        len_sum = sum(length.values())\n",
    "        for a in length.keys():\n",
    "            length[a] /= len_sum\n",
    "            \n",
    "        rem = 0\n",
    "        for att in level.keys():\n",
    "#             part_g = self.Gini(level[att])\n",
    "            part_g = self.Entropy(level[att])\n",
    "            rem += ( part_g * length[att] )\n",
    "        if len(sp.keys())==0:\n",
    "            print('Categorical only')\n",
    "            print(sp)\n",
    "            return -999999, {}\n",
    "        n = {'attr':attr, 'value':0,'childs':sp}\n",
    "        \n",
    "        H = self.Entropy(T)\n",
    "#         GR = (H - rem) / HD\n",
    "        GR = H - rem\n",
    "        return GR, n\n",
    "    \n",
    "    def continuous_split(self, attr, X_idx, data_X, target):\n",
    "\n",
    "        nums = [ data_X.loc[dx, attr] for dx in X_idx ] \n",
    "        up , down = max(nums), min(nums)\n",
    "\n",
    "        leng = up - down\n",
    "        div = int(leng / 20)\n",
    "        \n",
    "        threshold = []\n",
    "        if div == 0:\n",
    "            threshold.append(down)\n",
    "        else:\n",
    "            threshold = [i for i in range(int(down)+div, int(up), div)]\n",
    "#             threshold = [(up + down) / 2]    \n",
    "        max_GR = -999999\n",
    "        max_sp = {}\n",
    "        max_value = -1\n",
    "        T = {}\n",
    "        for dx in X_idx:\n",
    "            tar = target.loc[dx, 'Category']\n",
    "            if tar not in T:\n",
    "                T[tar] = 0\n",
    "            T[tar] += 1\n",
    "        H = self.Entropy(T)\n",
    "        signal = 0\n",
    "        for t in threshold:\n",
    "            level = {'bigger':{}, 'smaller':{}}\n",
    "            length = {'bigger':0, 'smaller':0}\n",
    "            sp = {'bigger':[], 'smaller':[]}\n",
    "            tmp = []\n",
    "            signal = 0\n",
    "            for dx in X_idx:\n",
    "                att = data_X.loc[dx, attr]\n",
    "                tmp.append(att)\n",
    "                if att == ' ?':\n",
    "                    continue\n",
    "                \n",
    "                tar = target.loc[dx, 'Category']\n",
    "                \n",
    "                if t < att:\n",
    "                    length['bigger'] += 1\n",
    "                    if tar not in level['bigger']:\n",
    "                        level['bigger'][tar] = 0\n",
    "                    level['bigger'][tar] += 1\n",
    "                    sp['bigger'].append(dx)\n",
    "                else:\n",
    "                    length['smaller'] += 1\n",
    "                    if tar not in level['smaller']:\n",
    "                        level['smaller'][tar] = 0\n",
    "                    level['smaller'][tar] += 1\n",
    "                    sp['smaller'].append(dx)\n",
    "                    \n",
    "            if ( length['bigger'] == 0 or length['smaller'] == 0 ):\n",
    "                continue\n",
    "            \n",
    "            HD = self.Entropy(length)\n",
    "            \n",
    "            len_sum = sum(length.values())\n",
    "            for a in length.keys():\n",
    "                length[a] /= len_sum\n",
    "            \n",
    "            rem = 0\n",
    "            for att in level.keys():\n",
    "#                 part_g = self.Gini(level[att])\n",
    "                part_g = self.Entropy(level[att])\n",
    "\n",
    "                rem += ( part_g * length[att] )\n",
    "        \n",
    "#             GR = (H - rem) / HD\n",
    "            GR = H - rem\n",
    "            if GR > max_GR:\n",
    "                max_GR = GR\n",
    "                max_sp = sp\n",
    "                max_value = t\n",
    "        if len(max_sp) == 0:\n",
    "            return -999999, {}\n",
    "        \n",
    "        n = {'attr':attr, 'value':max_value,'childs':max_sp}\n",
    "        return max_GR, n\n",
    "    \n",
    "    def select_feature(self, X_idx, data_X, target):\n",
    "        max_gr = -999999\n",
    "        max_node = {}\n",
    "        for attr in data_X:\n",
    "            if attr == 'Id':\n",
    "                continue\n",
    "            typ = data_X.loc[:, attr]\n",
    "            \n",
    "            if type(typ.iloc[0]) == np.str:\n",
    "                GR, nod = self.categorical_split(attr, X_idx, data_X, target)\n",
    "            else:\n",
    "                GR, nod = self.continuous_split(attr, X_idx, data_X, target)\n",
    "\n",
    "            if (GR > max_gr):\n",
    "                max_node = nod\n",
    "                max_gr = GR\n",
    "        return max_node\n",
    "\n",
    "    def terminal(self, nod, X_idx):\n",
    "        outcome = []\n",
    "        for idx in X_idx:\n",
    "            outcome.append(data_Y.loc[idx, 'Category'])\n",
    "        out_class = max(set(outcome), key=outcome.count)\n",
    "        nod['class_'] = out_class\n",
    "        \n",
    "    def check_terminal(self, nod, X_idx, data_X, data_Y):\n",
    "        a = data_Y.loc[X_idx, 'Category']\n",
    "        if len(set(a)) == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def split(self, nod, X_idx, data_X, data_Y):\n",
    "        #check whether all category are the same\n",
    "        if self.check_terminal(nod, X_idx, data_X, data_Y):\n",
    "            self.terminal(nod, X_idx)\n",
    "            return\n",
    "        \n",
    "        #check whether there are sufficient attribute to divide\n",
    "        if len(data_X.columns) == 1:\n",
    "            self.terminal(nod, X_idx)\n",
    "            return\n",
    "\n",
    "        #pre-pruning\n",
    "        if len(X_idx) <= self.min_num_of_node:\n",
    "            self.terminal(nod, X_idx)\n",
    "            return\n",
    "        #pre-pruning\n",
    "        if nod['depth'] >= self.max_depth:\n",
    "            self.terminal(nod, X_idx)\n",
    "            return\n",
    "        #check whether all attribute are the same (entropy=0)\n",
    "        \n",
    "        tmp_nod = self.select_feature(X_idx, data_X, data_Y)\n",
    "        #cannot split (all value in a feature are the same)\n",
    "        if len(tmp_nod.keys())==0:\n",
    "            self.terminal(nod, X_idx)\n",
    "            return\n",
    "        \n",
    "        #pop class\n",
    "        p = []\n",
    "        for dx in X_idx:\n",
    "            p.append( data_Y.loc[dx, 'Category'] )\n",
    "        if p.count(0) == p.count(1):\n",
    "            pop_class = \"\"\n",
    "        else:\n",
    "            pop_class = max(set(p), key = p.count)\n",
    "        \n",
    "        nod['class_'] = pop_class\n",
    "        nod['attr'] = tmp_nod['attr']\n",
    "        nod['childs'] = tmp_nod['childs']\n",
    "        nod['value'] = tmp_nod['value']\n",
    "        \n",
    "        if len(nod['childs']) == 0:\n",
    "            self.terminal(nod, X_idx)\n",
    "            return\n",
    "        \n",
    "        data_X_tmp = data_X.drop(nod['attr'], axis=1)\n",
    "        \n",
    "        for att in nod['childs'].keys():\n",
    "            d = nod['depth']+1\n",
    "            nod['child_node'][att] = {'attr':\"\", 'value':0, 'childs':{}, 'child_node':{}, 'depth':d, 'class_':\"\"}\n",
    "            self.split(nod['child_node'][att], nod['childs'][att], data_X_tmp, data_Y)\n",
    "        return\n",
    "            \n",
    "    def build_tree(self, data_X, X_idx, data_Y):\n",
    "        if 'Id' in data_X.columns:\n",
    "            data_X = data_X.drop(columns='Id')\n",
    "        self.root= {'attr':\"\", 'value':0, 'childs':{}, 'child_node':{}, 'depth':0, 'class_':\"\"}\n",
    "        \n",
    "        self.split(self.root, X_idx, data_X, data_Y)\n",
    "        return self.root\n",
    "    \n",
    "    def post_prune(self, nod, valid_set, X_idx):\n",
    "        if len(nod['childs']) ==0:\n",
    "            return\n",
    "        \n",
    "        #popular class\n",
    "        pop_class = nod['class_']\n",
    "        #iterate child node\n",
    "        attr = nod['attr']\n",
    "        childs_error = 0\n",
    "        seg_idx = {}\n",
    "        for att in nod['child_node'].keys():\n",
    "            p_class = nod['child_node'][att]['class_']\n",
    "            if p_class == \"\":\n",
    "                continue\n",
    "            \n",
    "            v = nod['value']\n",
    "            if att=='bigger' or att=='smaller':\n",
    "                if att == 'bigger':\n",
    "                    s = valid_set.loc[ valid_set[attr] > v]\n",
    "                    seg_idx[att] = list(s.index)\n",
    "                if att == 'smaller':\n",
    "                    s = valid_set.loc[ valid_set[attr] <= v]\n",
    "                    seg_idx[att] = list(s.index)\n",
    "            else:\n",
    "                #categorical\n",
    "                s = valid_set.loc[ valid_set[attr] == att ]\n",
    "                seg_idx[att] = list(s.index)\n",
    "            \n",
    "            for row in seg_idx[att]:\n",
    "                if data_Y.loc[row, 'Category'] != p_class:\n",
    "                    childs_error += 1\n",
    "        \n",
    "        parent_error = 0\n",
    "        for dx in X_idx:\n",
    "            if data_Y.loc[dx, 'Category'] != pop_class:\n",
    "                parent_error += 1\n",
    "        \n",
    "        if childs_error >= parent_error:\n",
    "            #prune\n",
    "            nod['child_node'] = {}\n",
    "            nod['childs'] = {}\n",
    "        else:\n",
    "            tmp = valid_set.drop(columns = attr)\n",
    "            for att in nod['child_node'].keys():\n",
    "                self.post_prune(nod['child_node'][att], tmp, seg_idx[att])\n",
    "                   \n",
    "    def predict(self, nod, row, data_Y, print_time):\n",
    "        if len(nod['child_node']) == 0:\n",
    "            if print_time > 0:\n",
    "                print('reach leaf node, stop at level {1} and the predicted value is {0}'.format(nod['class_'], nod['depth']))\n",
    "                print('The real value is {0}.'.format( data_Y.loc[row['Id'], 'Category'] ))\n",
    "                print()\n",
    "            return nod['class_']\n",
    "        else: \n",
    "            sub_attr = row[nod['attr']]\n",
    "            \n",
    "            if type(sub_attr) == str:\n",
    "                if sub_attr not in nod['child_node']:\n",
    "                    ran_class = random.randint(0, 1)\n",
    "                    if print_time > 0:\n",
    "                        print('This attribute is not in {0}, so randomly predicted value is {1}'.format(nod['attr'], ran_class))\n",
    "                        print()\n",
    "                    return ran_class\n",
    "                next_node = nod['child_node'][sub_attr]\n",
    "                if print_time > 0:\n",
    "                    print('In level {2} Split by feature {0}, the sample\\'s attribute in {0} is {1}'.format(nod['attr'], sub_attr, nod['depth']) )\n",
    "            else:\n",
    "                if nod['value'] < sub_attr:\n",
    "                    next_node = nod['child_node']['bigger']\n",
    "                    if print_time > 0:\n",
    "                        print('In level {2} Split by feature {0}, the sample\\'s attribute in {0} is larger than threshold {1}'.format(nod['attr'], nod['value'], nod['depth']), end='\\n' )\n",
    "\n",
    "                else:\n",
    "                    next_node = nod['child_node']['smaller']\n",
    "                    if print_time > 0:\n",
    "                        print('In level {2} Split by feature {0}, the sample\\'s attribute in {0} is smaller than threshold {1}'.format(nod['attr'], nod['value'], nod['depth']), end='\\n' )\n",
    "\n",
    "            ans = self.predict(next_node,row, data_Y,print_time)\n",
    "            return ans\n",
    "        \n",
    "    def pred(self, test_X, data_Y, print_or_not):\n",
    "        y = pd.DataFrame(columns=['Id', 'Category'])\n",
    "        if print_or_not == 1:\n",
    "            print_time = 10\n",
    "        else:\n",
    "            print_time = 0\n",
    "        for row in test_X.index:\n",
    "            if print_time > 0:\n",
    "                print('ID {0}: start prediction '.format(row))\n",
    "            ans = self.predict( self.root, test_X.loc[row, :], data_Y, print_time )\n",
    "            print_time -= 1\n",
    "            y.loc[row] = [test_X.loc[row,'Id'], ans]\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def print_tree(self, nod, data_X, data_Y):\n",
    "        if len(nod['child_node'])==0:\n",
    "            return\n",
    "        for b in range( nod['depth'] ):\n",
    "                print(' ', end='')\n",
    "        print('Split by {}'.format(nod['attr']))\n",
    "        print()\n",
    "        tmp = data_X.drop(columns=nod['attr'])\n",
    "        for a in nod['child_node'].keys():\n",
    "            for b in range( nod['depth'] ):\n",
    "                print(' ', end='')\n",
    "            print('attr {}'.format(a))\n",
    "            print()\n",
    "            for b in range( nod['depth'] ):\n",
    "                print(' ', end='')\n",
    "            print(nod['childs'][a])\n",
    "            for b in range( nod['depth'] ):\n",
    "                print(' ', end='')\n",
    "            print(data_Y.loc[nod['childs'][a], 'Category' ].values)\n",
    "            print()\n",
    "        \n",
    "            self.print_tree(nod['child_node'][a], tmp, data_Y)\n",
    "        return\n",
    "    \n",
    "    def acc(self, y, data_Y):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        confusion = pd.DataFrame(0, index= ['real 0', 'real 1'] , columns=['predict 0', 'predict 1'])\n",
    "        \n",
    "        for a in y.index:\n",
    "            if y.loc[a]['Category'] == data_Y.loc[a,'Category']:\n",
    "                correct += 1\n",
    "                \n",
    "                if y.loc[a]['Category'] == 1:\n",
    "                    confusion.iloc[1,1] += 1\n",
    "                else:\n",
    "                    confusion.iloc[0, 0] += 1\n",
    "            else:\n",
    "                if y.loc[a]['Category'] == 1:\n",
    "                    confusion.iloc[0, 1] += 1\n",
    "                else:\n",
    "                    confusion.iloc[1, 0] += 1\n",
    "            total += 1\n",
    "        \n",
    "        return correct / total, confusion\n",
    "    \n",
    "    def sensitivity(self, confusion):\n",
    "        sensitivity = {}\n",
    "        sensitivity['0'] = confusion.iloc[0, 0] / sum(confusion.iloc[0, :])\n",
    "        sensitivity['1'] = confusion.iloc[1, 1] / sum(confusion.iloc[1, :])\n",
    "        sen = pd.Series(sensitivity)\n",
    "        return sen\n",
    "    \n",
    "    def precision(self, confusion):\n",
    "        precision = {}\n",
    "        precision['0'] = confusion.iloc[0, 0] / sum(confusion.iloc[:, 0]) \n",
    "        precision['1'] = confusion.iloc[1, 1] / sum(confusion.iloc[:, 1]) \n",
    "        pre = pd.Series(precision)\n",
    "        return pre\n",
    "    def print_metric(self, confusion, acc, n):\n",
    "        \n",
    "        print('accuracy: {0}'.format(acc / n), end='\\n\\n')\n",
    "        print('confusion matrix: ')\n",
    "        print(confusion, end='\\n\\n')\n",
    "        print('sensitivity: ')\n",
    "        print(self.sensitivity(confusion), end='\\n\\n')\n",
    "        print('precision: ')    \n",
    "        print(self.precision(confusion), end='\\n\\n')\n",
    "        \n",
    "    def K_fold(self, data_X, data_Y, n_fold, idx):\n",
    "        total_idx = [i for i in data_X.index]\n",
    "\n",
    "        seg = []\n",
    "        fold_siz = int(data_X.shape[0] / n_fold )\n",
    "        now = 0\n",
    "        for i in range(n_fold):\n",
    "            seg.append(data_X.iloc[now : now + fold_siz,:])\n",
    "            now += fold_siz\n",
    "\n",
    "        valid_set = seg[idx]\n",
    "        train_set = pd.DataFrame(columns=data_X.columns)\n",
    "\n",
    "        for j in range(len(seg)):\n",
    "            if j != idx:\n",
    "                train_set = pd.concat( [seg[j], train_set], ignore_index=False )\n",
    "        return train_set, valid_set\n",
    "\n",
    "    \n",
    "    def Holdout_Validation(self, data_X, ratio):\n",
    "\n",
    "        fold_siz = int(data_X.shape[0] * ratio)\n",
    "\n",
    "        train_set = data_X.iloc[:fold_siz, : ]\n",
    "        valid_set = data_X.iloc[fold_siz:, : ]\n",
    "        \n",
    "        return train_set, valid_set\n",
    "        \n",
    "class RandomForest:\n",
    "    def __init__(self, tree_num):\n",
    "        self.tree_num = tree_num\n",
    "    def subsample(self, data_X, ratio = 1.0):\n",
    "        num = int(data_X.shape[0] * ratio)\n",
    "        s = random.choices( data_X.index, k=num)\n",
    "        ans = set(s)\n",
    "        return list(ans)\n",
    "    \n",
    "    def sample_feature(self, columns, ratio = 1.0):\n",
    "        num = int(data_X.shape[1] * ratio)\n",
    "        c = random.choices( data_X.columns, k=num)\n",
    "        c = list( set(c) )\n",
    "        if 'Id' not in c:\n",
    "            c.append('Id')\n",
    "        return c\n",
    "        \n",
    "    def predict(self, IDs, predictions):\n",
    "#         rows = predictions[0].index\n",
    "        rows = len(predictions[0])\n",
    "        \n",
    "        ans = pd.DataFrame(columns = ['Id', 'Category'])\n",
    "        ans['Id'] = IDs\n",
    "        ansc = []\n",
    "        for i in range(rows):\n",
    "            p = [ predictions[idx][i] for idx in range(len(predictions)) ]\n",
    "            ansp = max(set(p) , key=p.count)\n",
    "            ansc.append(ansp)\n",
    "        \n",
    "        ans['Category'] = ansc\n",
    "        return ans\n",
    "        \n",
    "    def build(self, data_X, data_Y, test_X):\n",
    "        T = DecisionTree()\n",
    "\n",
    "        preds = []\n",
    "        predss = []\n",
    "        \n",
    "        #K_fold\n",
    "#         print('===========================================')\n",
    "#         print('Start implementing k_fold:')\n",
    "#         print()\n",
    "\n",
    "#         n_fold = 3\n",
    "#         total_acc = 0\n",
    "#         total_confusion = pd.DataFrame(0, index= ['real 0', 'real 1'] , columns=['predict 0', 'predict 1'])\n",
    "        \n",
    "#         for idx in range(n_fold):\n",
    "            \n",
    "#             train_set, test_set = T.K_fold(data_X.iloc[:,:], data_Y, n_fold, idx)\n",
    "            \n",
    "#             for i in range(self.tree_num):\n",
    "#                 sample_X_idx = self.subsample(train_set, 0.7)\n",
    "#     #             sample_feature = self.sample_feature(train_set.columns[1:], 0.7)\n",
    "\n",
    "#                 tree = T.build_tree(train_set.loc[:,:], sample_X_idx, data_Y)\n",
    "\n",
    "#                 y = T.pred(test_set, data_Y, 0)\n",
    "#                 y = y.sort_values(by='Id')\n",
    "#                 preds.append(list(y['Category']))\n",
    "                \n",
    "#             test_set = test_set.sort_values(by='Id')\n",
    "#             IDs = test_set['Id']\n",
    "#             y = self.predict(IDs, preds)\n",
    "            \n",
    "#             accuracy, confusion = T.acc(y, data_Y)\n",
    "            \n",
    "#             total_confusion = total_confusion.add(confusion) \n",
    "#             total_acc += accuracy\n",
    "#         T.print_metric(total_confusion, total_acc, 3)\n",
    "        \n",
    "        #Holdout_validation\n",
    "        preds = []\n",
    "        print('-------------------------------------------')\n",
    "        print('Start implementing holdout_validation:')\n",
    "        print()\n",
    "\n",
    "        train_set, test_set = T.Holdout_Validation(data_X, 1.0)\n",
    "        idx = 1\n",
    "        \n",
    "        for i in range(self.tree_num):\n",
    "            sample_X_idx = self.subsample(train_set, 0.5)\n",
    "#             sample_feature = self.sample_feature(train_set.columns[1:], 0.7)\n",
    "            tree = T.build_tree(train_set.iloc[:,:], sample_X_idx, data_Y)\n",
    "            \n",
    "#             y = T.pred(test_set, data_Y, 0)\n",
    "#             y = y.sort_values(by='Id')\n",
    "#             preds.append(list(y['Category']))\n",
    "            \n",
    "            y1 = T.pred(test_X, data_Y, 0)\n",
    "            y1 = y1.sort_values(by='Id')\n",
    "            predss.append(list(y1['Category']))\n",
    "            \n",
    "#         test_set = test_set.sort_values(by='Id')\n",
    "#         IDs = test_set['Id']\n",
    "#         y = self.predict(IDs, preds)\n",
    "        \n",
    "#         accuracy, confusion = T.acc(y, data_Y)\n",
    "#         T.print_metric(confusion, accuracy, 1)\n",
    "        \n",
    "        IDs = test_X['Id']\n",
    "        y = self.predict(IDs, predss)\n",
    "        y.to_csv(\"test.csv\",index=False, sep=',')\n",
    "        print('save file finish')\n",
    "        \n",
    "data_X = pd.read_csv(\"data/X_train.csv\")\n",
    "data_Y = pd.read_csv(\"data/y_train.csv\")\n",
    "test_X = pd.read_csv(\"data/X_test.csv\")\n",
    "\n",
    "data_X = data_X.sample(frac=1)\n",
    "\n",
    "#pre-processing\n",
    "for att in data_X.columns:\n",
    "    tmp = data_X.loc[ data_X[att] == ' ?']\n",
    "    if tmp.empty:#dataframe is empty\n",
    "        continue\n",
    "    else:\n",
    "        f = data_X[att].value_counts()\n",
    "        most = f.index[0]\n",
    "        data_X.loc[tmp.index, att] = most\n",
    "\n",
    "\n",
    "R = RandomForest(100)\n",
    "print('===========================================')\n",
    "print('Model Random Forest:')\n",
    "R.build(data_X.iloc[:,:], data_Y, test_X)\n",
    "# # specified = [2, 4, 6, 7, 8, 9, 10, 14]\n",
    "\n",
    "# T = DecisionTree()\n",
    "# print('===========================================')\n",
    "\n",
    "# print('Model Decision Treee:')\n",
    "# #Implement k_fold\n",
    "# print('===========================================')\n",
    "# print('Start implementing k_fold')\n",
    "# print()\n",
    "# n_fold = 3\n",
    "# total_acc = 0\n",
    "# total_confusion = pd.DataFrame(0, index= ['real 0', 'real 1'] , columns=['predict 0', 'predict 1'])\n",
    "\n",
    "# for idx in range(n_fold):\n",
    "#     train_set, test_set = T.K_fold(data_X.iloc[:,:], data_Y, n_fold, idx)\n",
    "#     total_idx = [i for i in train_set.index]\n",
    "    \n",
    "#     T.build_tree(train_set, total_idx, data_Y)\n",
    "    \n",
    "#     y = T.pred(test_set, data_Y, 0)\n",
    "\n",
    "#     accuracy, confusion = T.acc(y, data_Y)\n",
    "#     total_confusion = total_confusion.add(confusion) \n",
    "#     total_acc += accuracy\n",
    "# #     print('{0} fold accuracy: {1}'.format(idx+1, accuracy))\n",
    "# T.print_metric(total_confusion, total_acc, 3)\n",
    "    \n",
    "    \n",
    "# #implement Holdout_validation\n",
    "# print('-------------------------------------------')\n",
    "# print('Start implementing Holdout_validation:')\n",
    "# print()\n",
    "# train_set, test_set = T.Holdout_Validation(data_X.iloc[:,:], 0.7)\n",
    "# total_idx = [i for i in train_set.index]\n",
    "\n",
    "# T.build_tree(train_set.iloc[:,:], total_idx, data_Y)\n",
    "# # print(\"Build finish\")\n",
    "\n",
    "# # T.post_prune(T.root, test_set, test_set.index)\n",
    "# # T.print_tree(T.root, train_set.iloc[:,:], data_Y)\n",
    "\n",
    "# # y = T.pred(train_set)\n",
    "# # accuracy = T.acc(y, data_Y)\n",
    "# # print(accuracy)\n",
    "# print('Printing prediction procedure of 10 samples:', end='\\n\\n')\n",
    "# y = T.pred(test_set, data_Y, 1)\n",
    "# accuracy , confusion = T.acc(y, data_Y)\n",
    "# T.print_metric(confusion, accuracy, 1)\n",
    "# # y = T.pred(test_X)\n",
    "# # y.to_csv(\"test.csv\",index=False, sep=',')\n",
    "# # print('save file finish')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\ops\\__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:32: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8394267329628546\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "specified = [3, 5, 6, 7, 8] #categorical\n",
    "\n",
    "data_X = pd.read_csv(\"X_train.csv\")\n",
    "data_Y = pd.read_csv(\"y_train.csv\")\n",
    "test_X = pd.read_csv(\"X_test.csv\")\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for att in data_X.columns:\n",
    "    if data_X.loc[ data_X[att] == ' ?'].empty:#dataframe is empty\n",
    "        continue\n",
    "    else:\n",
    "#         print(att)\n",
    "        data_X = data_X.drop(columns=att)\n",
    "for i in range(len(specified)):\n",
    "    b = list(set( data_X.iloc[:,specified[i]] ))\n",
    "    le.fit( b )\n",
    "    a = le.transform(data_X.iloc[:,specified[i]])\n",
    "    data_X.iloc[:,specified[i]] = a\n",
    "\n",
    "data_Y = data_Y.drop(columns='Id')\n",
    "data_X = data_X.drop(columns='Id')\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size = 0.3, random_state=1)\n",
    "clf = RandomForestClassifier(500)\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy: \",metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1\n",
      "0  0.8  0.3\n",
      "1  0.6  0.9\n",
      "     0    1\n",
      "0  0.4  0.2\n",
      "1  0.4  0.6\n",
      "     0    1\n",
      "0  0.4  0.1\n",
      "1  0.2  0.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_X = pd.read_csv(\"X_train.csv\")\n",
    "data_Y = pd.read_csv(\"y_train.csv\")\n",
    "test_X = pd.read_csv(\"X_test.csv\")\n",
    "\n",
    "# data_X = data_X.sample(frac=1)\n",
    "# data_X = data_X.drop(columns=['capital-gain', 'capital-loss'])\n",
    "# for num in data_X.columns:\n",
    "#     t = data_X[num].value_counts()\n",
    "#     print(t.index)\n",
    "\n",
    "confusion = pd.DataFrame({'0': [0.4, 0.4] , '1':[0.2, 0.6]})\n",
    "df = pd.DataFrame( {'0':[0.4, 0.2], '1':[0.1, 0.3]})\n",
    "print(confusion.add(df))\n",
    "print(confusion)\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
